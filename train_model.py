import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Attention, GlobalAveragePooling1D, Concatenate, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ Ù…Ø­Ø§ÙƒØ§Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Kaggle)
# -------------------------------------------------------
def load_real_data():
    # Ø­Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
    try:
        df = pd.read_csv('clinical_data.csv')
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Kaggle Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©.")
    except FileNotFoundError:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù CSVØŒ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©...")
        # ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­Ø§ÙƒÙŠ Ù‡ÙŠÙƒÙ„ÙŠØ© Ø¨ÙŠØ§Ù†Ø§Øª Sepsis ÙÙŠ Kaggle
        # (3000 Ù…Ø±ÙŠØ¶ØŒ ÙƒÙ„ Ù…Ø±ÙŠØ¶ Ù„Ø¯ÙŠÙ‡ 24-50 Ø³Ø§Ø¹Ø©)
        n_patients = 1000
        data = []
        for pid in range(n_patients):
            hours = 24
            is_sepsis = np.random.rand() > 0.8 # 20% Ù…Ø±Ø¶Ù‰ Ø®Ø·Ø±ÙŠÙ†
            
            base_hr = np.random.normal(80, 10)
            base_sbp = np.random.normal(120, 15)
            
            for h in range(hours):
                # Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø· Ø§Ù„ØªØ¯Ù‡ÙˆØ± Ù„Ù„Ù…Ø±Ø¶Ù‰ Ø§Ù„Ø®Ø·Ø±ÙŠÙ†
                trend = (h/hours) if is_sepsis else 0
                
                hr = base_hr + (trend * 30) + np.random.normal(0, 5)
                sbp = base_sbp - (trend * 20) + np.random.normal(0, 5)
                o2 = 98 - (trend * 10) + np.random.normal(0, 2)
                resp = 18 + (trend * 5) + np.random.normal(0, 2)
                
                # ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù€ Label ÙÙŠ Ø¢Ø®Ø± 6 Ø³Ø§Ø¹Ø§Øª
                label = 1 if (is_sepsis and h > 18) else 0
                
                data.append([pid, h, hr, sbp, o2, resp, label])
        
        df = pd.DataFrame(data, columns=['Patient_ID', 'Hour', 'HR', 'SBP', 'O2Sat', 'Resp', 'Label'])
        df.to_csv('clinical_data.csv', index=False) # Ø­ÙØ¸Ù‡Ø§ ÙƒÙ…Ù„Ù Ù„ØªØ³ØªØ®Ø¯Ù…Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
    
    return df

# 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø£ØµØ¹Ø¨ Ù…Ø±Ø­Ù„Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©)
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø³Ø·Ø­ Ø¥Ù„Ù‰ (Samples, TimeSteps, Features)
# -------------------------------------------------------
def preprocess_data(df, time_steps=24):
    print("â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ø³Ù„Ø§Ø³Ù„ Ø²Ù…Ù†ÙŠØ©...")
    
    # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©)
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    grouped = df.groupby('Patient_ID')
    X = []
    y = []
    
    for _, group in grouped:
        # Ù†Ø£Ø®Ø° Ø¢Ø®Ø± 24 Ø³Ø§Ø¹Ø© Ù„ÙƒÙ„ Ù…Ø±ÙŠØ¶
        if len(group) >= time_steps:
            # Ø§Ù„Ø®ØµØ§Ø¦Øµ: HR, SBP, O2Sat, Resp
            vitals = group[['HR', 'SBP', 'O2Sat', 'Resp']].values[-time_steps:]
            # Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù‡Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶ Ù…ØµØ§Ø¨ ÙÙŠ Ø¢Ø®Ø± Ø³Ø§Ø¹Ø©ØŸ
            label = group['Label'].values[-1]
            
            X.append(vitals)
            y.append(label)
            
    X = np.array(X)
    y = np.array(y)
    return X, y

# ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
df = load_real_data()
X, y = preprocess_data(df)

# ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Scaling)
scaler = StandardScaler()
# Ù†Ø­ØªØ§Ø¬ ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ù€ 2D Ù„Ù„ØªØ­Ø¬ÙŠÙ… Ø«Ù… Ø¥Ø¹Ø§Ø¯ØªÙ‡Ø§ Ù„Ù€ 3D
X_reshaped = X.reshape(-1, 4)
X_scaled = scaler.fit_transform(X_reshaped).reshape(-1, 24, 4)

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print(f"Dataset Shape: {X_train.shape}")

# 3. Ø¨Ù†Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„Ù‚ÙˆÙŠØ©)
# -------------------------------------------------------
inputs = Input(shape=(24, 4))
lstm_out = Bidirectional(LSTM(64, return_sequences=True))(inputs)
attention_layer = Attention(name='attention_weight')
context_vector = attention_layer([lstm_out, lstm_out])
concatenated = Concatenate()([lstm_out, context_vector])
gap = GlobalAveragePooling1D()(concatenated)
x = Dense(32, activation='relu')(gap)
x = Dropout(0.3)(x)
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])

print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# 4. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬
# -------------------------------------------------------
model.save('deepvital_model.h5')
print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³Ù…: deepvital_model.h5")

# Ø­ÙØ¸ Ø§Ù„Ù€ Scaler Ù„Ù†Ø³ØªØ®Ø¯Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„ØªÙƒÙˆÙ† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…ØªÙ†Ø§Ø³Ù‚Ø©)
import joblib
joblib.dump(scaler, 'scaler.pkl')
print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù€ Scaler.")